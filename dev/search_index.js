var documenterSearchIndex = {"docs":
[{"location":"custom/#Customizing-your-sampler","page":"Customizing your sampler","title":"Customizing your sampler","text":"","category":"section"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"This document describes how to extend DEMetropolis.jl with your own custom components. You can define custom stopping criteria, diagnostic checks, and proposal distributions (updates).","category":"page"},{"location":"custom/#Custom-Stopping-Criteria","page":"Customizing your sampler","title":"Custom Stopping Criteria","text":"","category":"section"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"To create a custom stopping criterion, you need to define a function that follows the AbstractMCMC interface, similar to r̂_stopping_criteria. The function will be called during sampling to determine when to stop. See the AbstractMCMC documentation for details.","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"The stopping criterion function has the following signature:","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"function your_stopping_criteria(\n    rng::AbstractRNG,\n    model::AbstractModel, \n    sampler::AbstractDifferentialEvolutionSampler,\n    samples::Vector{DifferentialEvolutionSample},\n    state::DifferentialEvolutionState,\n    iteration::Int;\n    kwargs...\n)","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"rng: Random number generator (required by AbstractMCMC interface, usually unused)\nmodel: The model being sampled (required by interface, usually unused)  \nsampler: The differential evolution sampler (required by interface, usually unused)\nsamples: Vector of all collected samples from all chains\nstate: Current sampler state (required by interface, usually unused)\niteration: Current iteration number\nkwargs...: Additional keyword arguments passed via AbstractMCMC.sample","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"The function should return true if sampling should stop, and false otherwise.","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"Here is an example of a very simple stopping criterion that stops sampling after a maximum number of iterations has been reached:","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"using DEMetropolis, AbstractMCMC\n\nfunction max_iterations_stopping(\n    rng::AbstractRNG,\n    model::AbstractModel,\n    sampler::AbstractDifferentialEvolutionSampler, \n    samples::Vector{DifferentialEvolutionSample{V, VV}},\n    state::DifferentialEvolutionState{T, V, VV, A},\n    iteration::Int;\n    max_iterations::Int = 10000,\n    kwargs...\n) where {T<:Real, V<:AbstractVector{T}, VV<:AbstractVector{V}, A<:AbstractDifferentialEvolutionAdaptiveState{T}}\n    if iteration >= max_iterations\n        println(\"Reached maximum iterations ($max_iterations), stopping.\")\n        return true\n    end\n    return false\nend\n\n# Usage with AbstractMCMC.sample\nusing LogDensityProblems\n\nmodel = LogDensityModel(your_log_density)\nsampler = setup_de_update()\n\nresult = sample(\n    rng, \n    model, \n    sampler, \n    max_iterations_stopping;\n    max_iterations = 5000,  # passed as keyword argument\n    n_chains = 4\n)","category":"page"},{"location":"custom/#Custom-Proposal-Distributions","page":"Customizing your sampler","title":"Custom Proposal Distributions","text":"","category":"section"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"You can create your own proposal distributions by defining a new sampler type that subtypes AbstractDifferentialEvolutionSampler and implementing the proposal method.","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"The method signature for the proposal is:","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"function proposal!(\n    state::DEMetropolis.DifferentialEvolutionState, \n    sampler::YourSampler, \n    current_state::Int\n)","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"state: The current state containing all chain positions, log-densities, and chain specific rngs\nsampler: An instance of your custom sampler struct\ncurrent_state: The index of the chain to be updated","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"The function should modify state.xₚ[current_state] = proposed_position and return a named tuple with at least (offset = hastings_correction) where:","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"proposed_position: The proposed new position (vector)\noffset: Hastings ratio correction in log-space (typically 0.0 for symmetric proposals)","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"Here is an example of a simple Metropolis-Hastings random walk update with a fixed step size:","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"using DEMetropolis, Distributions, Random\n\n# Define the struct for the sampler\nstruct MetropolisHastingsUpdate <: DEMetropolis.AbstractDifferentialEvolutionSampler\n    proposal_distribution::MvNormal\nend\n\n# Implement the proposal function\nfunction DEMetropolis.proposal!(\n    state::DEMetropolis.DifferentialEvolutionState,\n    sampler::MetropolisHastingsUpdate,\n    current_state::Int\n)\n    # Get the current position of this chain\n    x_current = state.x[current_state]\n    \n    # Propose a new point (stored in state) using a random walk\n    state.xₚ[current_state] .= x_current .+ rand(state.rngs[current_state], sampler.proposal_distribution)\n    \n    # The proposal is symmetric, so no Hastings correction needed\n    return (offset = 0.0)\nend","category":"page"},{"location":"custom/#Adaptive-Proposals-with-step_warmup","page":"Customizing your sampler","title":"Adaptive Proposals with step_warmup","text":"","category":"section"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"For proposals that require adaptation during warm-up, you need to implement the step_warmup as well. This is called during the warm-up phase. Unless you want your sampler to be always adaptive then you must implement step.","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"You'll also need to define adaptive state structures and methods. Here's an example of an adaptive Metropolis-Hastings sampler:","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"using AbstractMCMC, DEMetropolis\n# Define adaptive state\nstruct AdaptiveMetropolisState{T<:Real} <:DEMetropolis.AbstractDifferentialEvolutionAdaptiveState{T}\n    proposal_cov::Matrix{T}\n    adaptation_count::Int\n    running_mean::Vector{T}\n    running_cov::Matrix{T}\nend\n\n# Define the adaptive sampler  \nstruct AdaptiveMetropolisUpdate{T<:Real} <: DEMetropolis.AbstractDifferentialEvolutionSampler\n    initial_cov::Matrix{T}\n    adapt_after::Int\n    adapt_every::Int\n    adapt_scale::T\nend\n\n# Constructor\nfunction AdaptiveMetropolisUpdate(\n    n_params::Int;\n    initial_std::Float64 = 0.1,\n    adapt_after::Int = 200,\n    adapt_every::Int = 100,\n    adapt_scale::Float64 = 2.38^2\n)\n    initial_cov = (initial_std^2) * I(n_params)\n    return AdaptiveMetropolisUpdate{Float64}(initial_cov, adapt_after, adapt_every, adapt_scale / n_params)\nend\n\n# Initialize adaptive state\nfunction DEMetropolis.initialize_adaptive_state(sampler::AdaptiveMetropolisUpdate{T}, model_wrapper::AbstractMCMC.LogDensityModel, n_chains::Int) where {T}\n    n_params = dimension(model_wrapper.logdensity)\n    return AdaptiveMetropolisState{T}(\n        copy(sampler.initial_cov),\n        0,\n        zeros(T, n_params),\n        copy(sampler.initial_cov)\n    )\nend\n\n# Fix sampler (convert adaptive to non-adaptive)\nfunction DEMetropolis.fix_sampler(sampler::AdaptiveMetropolisUpdate{T}, adaptive_state::AdaptiveMetropolisState{T}) where {T}\n    return MetropolisHastingsUpdate(MvNormal(zeros(T, size(adaptive_state.proposal_cov, 1)), adaptive_state.proposal_cov))\nend\n\n# Proposal method (same as non-adaptive)\nfunction DEMetropolis.proposal!(\n    state::DEMetropolis.DifferentialEvolutionState,\n    sampler::AdaptiveMetropolisUpdate,\n    current_state::Int\n)\n    x_current = state.x[current_state]\n    # Use current proposal covariance from adaptive state\n    state.xₚ[current_state] .= rand(rng, MvNormal(x_current, state.adaptive_state.proposal_cov))\n    return (offset = 0.0)\nend\n\n# Adaptive step during warm-up\nfunction step_warmup(\n    rng::AbstractRNG,\n    model_wrapper::AbstractMCMC.LogDensityModel,\n    sampler::AdaptiveMetropolisUpdate{T},\n    state::DEMetropolis.DifferentialEvolutionState{T, AdaptiveMetropolisState{T}};\n    parallel::Bool = false,\n    kwargs...\n) where {T<:Real}\n    \n    # Perform regular step\n    sample, new_state = step(rng, model_wrapper, sampler, state; parallel = parallel, kwargs...)\n    \n    # Update adaptive parameters\n    adapt_state = new_state.adaptive_state\n    new_count = adapt_state.adaptation_count + 1\n    \n    # Only adapt after burn-in period and at specified intervals\n    if new_count > sampler.adapt_after && new_count % sampler.adapt_every == 0\n        \n        # Compute empirical covariance from current chain positions\n        positions = reduce(hcat, new_state.x)'  # Convert to matrix\n        empirical_cov = cov(positions)\n        \n        # Update proposal covariance with regularization\n        new_proposal_cov = sampler.adapt_scale * empirical_cov + 1e-6 * I\n        \n        # Update adaptive state\n        new_adaptive_state = AdaptiveMetropolisState{T}(\n            new_proposal_cov,\n            new_count,\n            adapt_state.running_mean,  # Could update these too\n            adapt_state.running_cov\n        )\n        \n        return sample, update_state(new_state; adaptive_state = new_adaptive_state)\n    else\n        # Just update the count\n        new_adaptive_state = AdaptiveMetropolisState{T}(\n            adapt_state.proposal_cov,\n            new_count,\n            adapt_state.running_mean,\n            adapt_state.running_cov\n        )\n        return sample, update_state(new_state; adaptive_state = new_adaptive_state)\n    end\nend","category":"page"},{"location":"custom/#Example:-Using-Custom-Components","page":"Customizing your sampler","title":"Example: Using Custom Components","text":"","category":"section"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"Here is a complete example that shows how to use custom components with the new AbstractMCMC interface:","category":"page"},{"location":"custom/","page":"Customizing your sampler","title":"Customizing your sampler","text":"using Distributions, TransformedLogDensities, LinearAlgebra, TransformVariables, Plots\n\n# Set up a simple log-density to sample from (a 2D standard normal distribution)\n\nld = TransformedLogDensity(as(Array, 2), x -> -sum(x.^2) / 2)\ndimension(ld) = 2\nmodel = AbstractMCMC.LogDensityModel(ld)\n\n# Create custom samplers\nsimple_mh = MetropolisHastingsUpdate(MvNormal([0.0, 0.0], 0.1 * I))\nadaptive_mh = AdaptiveMetropolisUpdate(2; initial_std = 0.1)\n\n# Create a composite sampler scheme\nmy_sampler_scheme = setup_sampler_scheme(\n    simple_mh, \n    adaptive_mh;\n    w = [0.3, 0.7]  # Use adaptive sampler 70% of the time\n)\n\n# Sample using AbstractMCMC.sample \nresult = sample(\n    Random.default_rng(),\n    model,\n    my_sampler_scheme,\n    5000;\n    n_chains = 6,\n    num_warmup = 10000, #adaptive steps\n    memory = true,\n    parallel = false,\n    chain_type = DifferentialEvolutionOutput\n)\n\nplot(result.samples[:, :, 1])\nplot(result.samples[:, :, 2])","category":"page"},{"location":"#DEMetropolis-Documentation","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"","category":"section"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"Tools for sampling from log-densities using differential evolution algorithms.","category":"page"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"See Sampling from multimodal distributions and Customizing your sampler to get started.","category":"page"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"This package is built upon AbstractMCMC.jl so log-densities should be constructed using that package, and can be used with TransformVariables.jl or Bijectors.jl to control the parameter space.","category":"page"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"The other key dependency is Distributions.jl. Almost every parameter in proposals given here are defined via customizable univariate distributions. Values that are fixed are specified via a Dirac distribution, though in the API these can be specified with any real value. As a warning there are some checks on the given distributions, but in the interest of flexibility it is up to the user to ensure that they are suitable for the given parameter. You can disable any checking of your provided distributions with ; check_args = false if you really want to ruin your sampler efficiency. Distributions can optionally be used to define your log-density, as in the examples given here. ","category":"page"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"As far as I am aware, there is one other package that implements differential evolution MCMC in Julia, DifferentialEvolutionMCMC.jl. I opted to implement my own version as I wanted a more flexible API and the subsampling scheme from DREAM. That's not to discredit DifferentialEvolutionMCMC.jl, it has many features this package does not, such as being able to work on optimization problems and parameter blocking.","category":"page"},{"location":"#Main-features","page":"DEMetropolis Documentation","title":"Main features","text":"","category":"section"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"Original differential evolution, snooker, and adaptive subspace sampling (i.e. from DREAM) updates\nOptional parallel tempering (no swaps yet, information is shared by the DE updates!) and annealing\nComposite samplers, can combine any of the implemented updates (in future I'll wrap other abstractMCMC based samplers)\nEasy to implement your own updates!\nCan output in MCMCChains format, though you use multiple sampling chains (i.e. chains of the DE-chains) these will all be appended together","category":"page"},{"location":"#Next-Steps","page":"DEMetropolis Documentation","title":"Next Steps","text":"","category":"section"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"A few plans for this package, feel free to suggest features or improvements via issues:","category":"page"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"Implement multi-try and delayed rejection DREAM, I avoided these so far since I have been using these samplers for costly log-densities with relatively few parameters, such as one that solve an ODE.\nAdditional diagnostic checks and adaptive schemes.","category":"page"},{"location":"#Contents","page":"DEMetropolis Documentation","title":"Contents","text":"","category":"section"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"","category":"page"},{"location":"#Functions","page":"DEMetropolis Documentation","title":"Functions","text":"","category":"section"},{"location":"#Implemented-Sampling-Schemes","page":"DEMetropolis Documentation","title":"Implemented Sampling Schemes","text":"","category":"section"},{"location":"#DEMetropolis.deMC","page":"DEMetropolis Documentation","title":"DEMetropolis.deMC","text":"deMC(model_wrapper, n_its; kwargs...)\n\nRun the Differential Evolution Markov Chain (DE-MC) sampler proposed by ter Braak (2006).\n\nThis sampler uses differential evolution updates with optional switching between two scaling factors (γ₁ and γ₂) to enable mode switching. The algorithm runs for a fixed number of iterations with optional burn-in.\n\nThis implementation varies slightly from the original: updates within a population occur based on the previous positions to enable easy parallelization.\n\nSee doi.org/10.1007/s11222-006-8769-1 for more information.\n\nArguments\n\nmodel_wrapper: LogDensityModel containing the target log-density function\nn_its: Number of sampling iterations per chain\n\nKeyword Arguments\n\nn_burnin: Number of burn-in iterations. Defaults to n_its * 5.\nn_chains: Number of chains. Defaults to max(dimension(ld) * 2, 3).\ninitial_state: Initial states for the chains. Defaults to random initialization.\nN₀: Size of initial population for memory-based sampling. Defaults to n_chains.\nmemory: Use memory-based sampling (true) or memoryless (false). Defaults to false.\nmemory_size: Maximum number of positions retained per chain in memory during initialization. Defaults to n_its + n_burnin.\nmemory_refill: When memory is full, overwrite from the start (cyclic) if true. Defaults to true (forwarded via keyword arguments).\nmemory_thin_interval: If > 0, only every memory_thin_interval-th accepted state is stored in memory. Defaults to 0 (forwarded via keyword arguments).\nsave_burnt: Save burn-in samples in output. Defaults to false.\nparallel: Run chains in parallel using threading. Defaults to false.\nsilent: Suppress informational initialization logs when true. Defaults to false (forwarded via keyword arguments).\nrng: Random number generator. Defaults to default_rng().\nthin: Thinning interval for saved samples. Defaults to 1.\nγ₁: Primary scaling factor. Defaults to 2.38 / sqrt(2 * dim).\nγ₂: Secondary scaling factor for mode switching. Defaults to 1.0.\np_γ₂: Probability of using γ₂. Defaults to 0.1.\nβ: Noise distribution. Defaults to Uniform(-1e-4, 1e-4).\nn_hot_chains: Number of hot chains for parallel tempering. Defaults to 0 (no parallel tempering).\nmax_temp_pt: Maximum temperature for parallel tempering. Defaults to 2*sqrt(dimension).\nmax_temp_sa: Maximum temperature for simulated annealing. Defaults to max_temp_pt.\nα: Temperature ladder spacing parameter. Defaults to 1.0.\nannealing: Whether to use simulated annealing. Defaults to false.\nannealing_steps: Number of annealing steps. Defaults to 0 or the number of warmup-steps (when using AbstractMCMC.sample).\ntemperature_ladder: Pre-defined temperature ladder. Defaults to automatic creation based on other parameters.\nchain_type: Type of chain to return (e.g., Any, DifferentialEvolutionOutput, MCMCChains.Chains). Defaults to DifferentialEvolutionOutput.\nsave_final_state: Whether to return the final state along with samples, if true the output will be (samples::chaintype, finalstate). Defaults to false.\nkwargs...: Additional keyword arguments passed to AbstractMCMC.sample and the internal initialization step (e.g., memory_refill, memory_thin_interval, silent). See AbstractMCMC documentation.\n\nReturns\n\nNamed tuple containing samples and optionally burn-in samples\n\nExample\n\nusing DEMetropolis, Random, Distributions\n\n# Define a simple log-density function\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\n\n# Run differential evolution MCMC\nresult = deMC(model_wrapper, 1000; n_chains = 10, parallel = false)\n\nSee also deMCzs, DREAMz, setup_de_update.\n\n\n\n\n\n","category":"function"},{"location":"#DEMetropolis.deMCzs","page":"DEMetropolis Documentation","title":"DEMetropolis.deMCzs","text":"deMCzs(model_wrapper, check_every; kwargs...)\n\nRun the Differential Evolution Markov Chain with snooker update and historic sampling (DE-MCzs) sampler.\n\nThis adaptive sampler runs until convergence (measured by R̂ < maximum_R̂) or a maximum number of epochs. It combines DE updates with optional snooker moves and uses memory-based sampling to efficiently handle high-dimensional problems with fewer chains.\n\nProposed by ter Braak and Vrugt (2008), see doi.org/10.1007/s11222-008-9104-9.\n\nArguments\n\nmodel_wrapper: LogDensityModel containing the target log-density function\ncheck_every: Number of iterations per chain per convergence check\n\nKeyword Arguments\n\nwarmup_epochs: Number of warm-up epochs before convergence checking. Defaults to 5.\nepoch_limit: Maximum number of total epochs. Defaults to 20.\nmaximum_R̂: Convergence threshold for Gelman-Rubin diagnostic. Defaults to 1.2.\nn_chains: Number of chains. Defaults to max(dimension(ld) * 2, 3).\nN₀: Size of initial population for memory-based sampling. Defaults to n_chains * 2.\ninitial_state: Initial population state. Defaults to random initialization.\nmemory: Use memory-based sampling. Defaults to true.\nsave_burnt: Save warm-up samples in output. Defaults to true.\nparallel: Run chains in parallel using threading. Defaults to false.\nrng: Random number generator. Defaults to default_rng().\nmemory_size: Maximum number of positions retained per chain in memory during initialization. Defaults to check_every * 10.\nmemory_refill: When memory is full, overwrite from the start (cyclic) if true. Defaults to true.\nmemory_thin_interval: If > 0, only every memory_thin_interval-th accepted state is stored in memory. Defaults to 0 (forwarded via keyword arguments).\nsilent: Suppress informational initialization logs when true. Defaults to false (forwarded via keyword arguments).\nγ: Scaling factor for DE updates. Defaults to 2.38 / sqrt(2 * dim).\nγₛ: Scaling factor for snooker updates. Defaults to 2.38 / sqrt(2).\np_snooker: Probability of snooker moves. Defaults to 0.1.\nβ: Noise distribution for DE updates. Defaults to Uniform(-1e-4, 1e-4).\nthin: Thinning interval for saved samples. Defaults to 1.\nn_hot_chains: Number of hot chains for parallel tempering. Defaults to 0 (no parallel tempering).\nmax_temp_pt: Maximum temperature for parallel tempering. Defaults to 2*sqrt(dimension).\nmax_temp_sa: Maximum temperature for simulated annealing. Defaults to max_temp_pt.\nα: Temperature ladder spacing parameter. Defaults to 1.0.\nannealing: Whether to use simulated annealing. Defaults to false.\nannealing_steps: Number of annealing steps. Defaults to 0 or the number of warmup-steps (when using AbstractMCMC.sample).\ntemperature_ladder: Pre-defined temperature ladder. Defaults to automatic creation based on other parameters.\nchain_type: Type of chain to return (e.g., Any, DifferentialEvolutionOutput, MCMCChains.Chains). Defaults to DifferentialEvolutionOutput.\nsave_final_state: Whether to return the final state along with samples, if true the output will be (samples::chaintype, finalstate). Defaults to false.\nkwargs...: Additional keyword arguments passed to AbstractMCMC.sample and the internal initialization step (e.g., memory_thin_interval, silent). See AbstractMCMC documentation.\n\nReturns\n\nNamed tuple containing samples, sampler scheme, and optionally burn-in samples\n\nExample\n\nusing DEMetropolis, Random, Distributions\n\n# Define a simple log-density function\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\n\n# Run adaptive differential evolution MCMC with convergence checking\nresult = deMCzs(model_wrapper, 1000; n_chains = 3, maximum_R̂ = 1.1)\n\nSee also deMC, DREAMz, r̂_stopping_criteria.\n\n\n\n\n\n","category":"function"},{"location":"#DEMetropolis.DREAMz","page":"DEMetropolis Documentation","title":"DEMetropolis.DREAMz","text":"DREAMz(model_wrapper, check_every; kwargs...)\n\nRun the Differential Evolution Adaptive Metropolis (DREAMz) sampler.\n\nThis advanced adaptive sampler runs until convergence and uses subspace sampling with adaptive crossover probabilities. It can switch between scaling factors and includes outlier chain detection/replacement. The algorithm adapts during warm-up and can use memory-based sampling for efficiency.\n\nBased on Vrugt et al. (2009), see doi.org/10.1515/IJNSNS.2009.10.3.273.\n\nArguments\n\nmodel_wrapper: LogDensityModel containing the target log-density function\ncheck_every: Number of iterations per chain per convergence check\n\nKeyword Arguments\n\nwarmup_epochs: Number of warm-up epochs for adaptation. Defaults to 5.\nepoch_limit: Maximum number of total epochs. Defaults to 20.\nmaximum_R̂: Convergence threshold for Gelman-Rubin diagnostic. Defaults to 1.2.\nn_chains: Number of chains. Defaults to max(dimension(ld) * 2, 3).\nN₀: Size of initial population. Defaults to n_chains * 2.\ninitial_state: Initial population state. Defaults to random initialization.\nmemory: Use memory-based sampling (true) or memoryless DREAM (false). Defaults to true.\nsave_burnt: Save warm-up samples in output. Defaults to true.\nparallel: Run chains in parallel using threading. Defaults to false.\nrng: Random number generator. Defaults to default_rng().\nmemory_size: Maximum number of positions retained per chain in memory during initialization. Defaults to check_every * 10.\nmemory_refill: When memory is full, overwrite from the start (cyclic) if true. Defaults to true.\nmemory_thin_interval: If > 0, only every memory_thin_interval-th accepted state is stored in memory. Defaults to 0 (forwarded via keyword arguments).\nsilent: Suppress informational initialization logs when true. Defaults to false (forwarded via keyword arguments).\nγ₁: Primary scaling factor for subspace updates. Defaults to adaptive.\nγ₂: Secondary scaling factor. Defaults to 1.0.\np_γ₂: Probability of using γ₂. Defaults to 0.2.\nn_cr: Number of crossover probabilities for adaptation. Defaults to 3.\ncr₁: Crossover probability for γ₁. Defaults to adaptive.\ncr₂: Crossover probability for γ₂. Defaults to adaptive.\nϵ: Additive noise distribution. Defaults to Uniform(-1e-4, 1e-4).\ne: Multiplicative noise distribution. Defaults to Normal(0.0, 1e-2).\nδ: Number of difference vectors distribution. Defaults to DiscreteUniform(1, 3).\nthin: Thinning interval for saved samples. Defaults to 1.\nn_hot_chains: Number of hot chains for parallel tempering. Defaults to 0 (no parallel tempering).\nmax_temp_pt: Maximum temperature for parallel tempering. Defaults to 2*sqrt(dimension).\nmax_temp_sa: Maximum temperature for simulated annealing. Defaults to max_temp_pt.\nα: Temperature ladder spacing parameter. Defaults to 1.0.\nannealing: Whether to use simulated annealing. Defaults to false.\nannealing_steps: Number of annealing steps. Defaults to 0 or the number of warmup-steps (when using AbstractMCMC.sample).\nchain_type: Type of chain to return (e.g., Any, DifferentialEvolutionOutput, MCMCChains.Chains). Defaults to DifferentialEvolutionOutput.\nsave_final_state: Whether to return the final state along with samples, if true the output will be (samples::chaintype, finalstate). Defaults to false.\nkwargs...: Additional keyword arguments passed to AbstractMCMC.sample and the internal initialization step (e.g., memory_thin_interval, silent). See AbstractMCMC documentation.\n\nReturns\n\nNamed tuple containing samples, sampler scheme, and optionally burn-in samples\n\nExample\n\nusing DEMetropolis, Random, Distributions\n\n# Define a simple log-density function\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\n\n# Run DREAM with subspace sampling\nresult = DREAMz(model_wrapper, 1000; n_chains = 10, memory = false)\n\nSee also deMC, deMCzs, setup_subspace_sampling.\n\n\n\n\n\n","category":"function"},{"location":"#Setup-Functions","page":"DEMetropolis Documentation","title":"Setup Functions","text":"","category":"section"},{"location":"#DEMetropolis.setup_sampler_scheme","page":"DEMetropolis Documentation","title":"DEMetropolis.setup_sampler_scheme","text":"Create a composite sampler scheme from multiple differential evolution update steps.\n\nThe update method used in each iteration for each chain is randomly selected from the provided update steps according to their weights. This allows combining different sampling strategies (e.g., DE updates with snooker updates) in a single sampler.\n\nArguments\n\nupdates...: One or more differential evolution sampler objects created by functions like setup_de_update, setup_snooker_update, setup_subspace_sampling.\n\nKeyword Arguments\n\nw: Vector of weights for each update step. If not provided, all updates are chosen with equal probability. Weights must be non-negative and will be automatically normalized.\n\nReturns\n\nA DifferentialEvolutionCompositeSampler that can be used with AbstractMCMC.sample.\n\nExamples\n\nusing DEMetropolis\n\n# Only snooker updates\nsampler1 = setup_sampler_scheme(setup_snooker_update())\n\n# DE and Snooker with equal probability\nsampler2 = setup_sampler_scheme(setup_de_update(), setup_snooker_update())\n\n# Snooker 10% of the time, DE 90% of the time\nsampler3 = setup_sampler_scheme(setup_de_update(), setup_snooker_update(); w = [0.9, 0.1])\n\nSee also setup_de_update, setup_snooker_update, setup_subspace_sampling.\n\n\n\n\n\n","category":"function"},{"location":"#DEMetropolis.setup_de_update","page":"DEMetropolis Documentation","title":"DEMetropolis.setup_de_update","text":"Set up a Differential Evolution (DE) update step for MCMC sampling.\n\nCreates a sampler that proposes new states by adding scaled difference vectors between randomly selected chains plus small noise. This is the core update mechanism from the original DE-MC algorithm by ter Braak (2006).\n\nSee doi.org/10.1007/s11222-006-8769-1 for more information.\n\nKeyword Arguments\n\nγ: Scaling factor for the difference vector. Can be a Real (fixed value), a UnivariateDistribution (random scaling), or nothing (automatic based on n_dims). Defaults to nothing.\nβ: Distribution for small noise added to proposals. Must be a univariate continuous distribution. Defaults to Uniform(-1e-4, 1e-4).\nn_dims: Problem dimension used for automatic γ selection. If > 0 and γ is nothing, sets γ to the theoretically optimal 2.38 / sqrt(2 * n_dims). If ≤ 0, uses Uniform(0.8, 1.2). Defaults to 0.\ncheck_args: Whether to validate input distributions. Defaults to true.\n\nReturns\n\nA DifferentialEvolutionSampler that can be used with setup_sampler_scheme or step or sample from AbstractMCMC.\n\nExample\n\nusing DEMetropolis, Distributions\n\n# Setup differential evolution update with custom parameters\nde_update = setup_de_update(γ = 1.0, β = Normal(0.0, 0.01))\n\nSee also setup_snooker_update, setup_subspace_sampling, setup_sampler_scheme.\n\n\n\n\n\n","category":"function"},{"location":"#DEMetropolis.setup_snooker_update","page":"DEMetropolis Documentation","title":"DEMetropolis.setup_snooker_update","text":"Set up a Snooker update step for MCMC sampling.\n\nCreates a sampler that proposes moves along the line connecting the current position to a projection point, scaled by the difference between two other randomly selected chains. This update can help with sampling from distributions with complex geometries by making larger moves in effective directions.\n\nSee doi.org/10.1007/s11222-008-9104-9 for more information.\n\nKeyword Arguments\n\nγ: Scaling factor for the projection. Can be a Real (fixed value), a UnivariateDistribution (random scaling), or nothing (automatic based on deterministic_γ). Defaults to nothing.\ndeterministic_γ: When γ is nothing, determines the automatic value. If true, uses the theoretically optimal 2.38 / sqrt(2). If false, uses Uniform(0.8, 1.2). Defaults to true.\ncheck_args: Whether to validate input distributions. Defaults to true.\n\nReturns\n\nA DifferentialEvolutionSnookerSampler that can be used with setup_sampler_scheme or step or sample from AbstractMCMC.\n\nExample\n\nusing DEMetropolis, Distributions\n\n# Setup snooker update with custom gamma distribution\nsnooker_update = setup_snooker_update(γ = Uniform(0.1, 2.0))\n\nSee also setup_de_update, setup_subspace_sampling, setup_sampler_scheme.\n\n\n\n\n\n","category":"function"},{"location":"#DEMetropolis.setup_subspace_sampling","page":"DEMetropolis Documentation","title":"DEMetropolis.setup_subspace_sampling","text":"Set up a Subspace Sampling (DREAM-like) update step for MCMC sampling.\n\nCreates a sampler that updates only a random subset of parameters in each iteration, using multiple scaled difference vectors. The crossover probability determines which parameters to update and can be adapted during warm-up for improved efficiency.\n\nSee doi.org/10.1515/IJNSNS.2009.10.3.273 for more information.\n\nKeyword Arguments\n\nγ: Scaling factor for the difference vector sum. If nothing (default), uses the adaptive formula 2.38 / sqrt(2 * δ * d) where d is the number of updated dimensions. If a Real is provided, uses that fixed value throughout sampling.\ncr: Crossover probability for parameter selection. Can be a Real (fixed probability), nothing (adaptive using n_cr values), or a UnivariateDistribution. If cr is a DiscreteNonParametric then it those values can also be adapted. Defaults to nothing.\nn_cr: Number of crossover probabilities to adapt between when cr is nothing. Higher values allow more fine-tuned adaptation. Defaults to 3.\nδ: Number of difference vectors to sum. Can be an Integer (fixed) or a DiscreteUnivariateDistribution (random). Defaults to DiscreteUniform(1, 3).\nϵ: Distribution for small additive noise in the selected subspace. Defaults to Uniform(-1e-4, 1e-4).\ne: Distribution for multiplicative noise (1 + e) applied to the difference vector sum. Defaults to Normal(0.0, 1e-2).\ncheck_args: Whether to validate input distributions. Defaults to true.\n\nReturns\n\nA subspace sampler that can be used with setup_sampler_scheme or step or sample from AbstractMCMC.\n\nExample\n\nusing DEMetropolis, Distributions\n\n# Setup subspace sampling with custom crossover rate and delta\nsubspace_config = setup_subspace_sampling(cr = Beta(1, 2), δ = 2)\n\nSee also setup_de_update, setup_snooker_update, setup_sampler_scheme.\n\n\n\n\n\n","category":"function"},{"location":"#Core-Sampling-Functions","page":"DEMetropolis Documentation","title":"Core Sampling Functions","text":"","category":"section"},{"location":"#AbstractMCMC.step","page":"DEMetropolis Documentation","title":"AbstractMCMC.step","text":"step(rng, model_wrapper, sampler, state; parallel=false, update_memory=true, kwargs...)\n\nPerform a single MCMC step using differential evolution sampling.\n\nThis is the core sampling function that proposes new states for all chains and accepts or rejects them according to the Metropolis criterion. For adaptive samplers, the function automatically fixes adaptive parameters before sampling.\n\nArguments\n\nrng: Random number generator\nmodel_wrapper: LogDensityModel containing the target log-density function\nsampler: Differential evolution sampler (any AbstractDifferentialEvolutionSampler)\nstate: Current state of all chains\n\nKeyword Arguments\n\nparallel: Whether to run chains in parallel using threading. Defaults to false. Advisable for slow models.\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Over writes memory options given at initialization.\nkwargs...: Additional keyword arguments passed to update functions (see https://turinglang.org/AbstractMCMC.jl/stable/api/#Common-keyword-arguments)\n\nReturns\n\nsample: DifferentialEvolutionSample containing new positions and log-densities\nnew_state: Updated state for the next iteration\n\nExample\n\nsample, new_state = step(rng, model, sampler, state; parallel=true)\n\nSee also step_warmup, sample from AbstractMCMC.\n\n\n\n\n\nstep(rng, model_wrapper, sampler; n_chains, memory=true, N₀, adapt=true, initial_position=nothing, parallel=false, kwargs...)\n\nInitialize differential evolution sampling by setting up chains and computing initial state.\n\nThis function serves as the entry point for differential evolution MCMC sampling. It handles chain initialization, memory setup for memory-based samplers, adaptive state initialization, and returns the initial sample and state that can be used with AbstractMCMC.sample.\n\nArguments\n\nrng: Random number generator\nmodel_wrapper: LogDensityModel containing the target log-density function\nsampler: Differential evolution sampler to use\n\nKeyword Arguments\n\nn_chains: Number of parallel chains. Defaults to max(2 * dimension, 3) for adequate mixing.\nn_hot_chains: Number of hot chains for parallel tempering. Defaults to 0 (no parallel tempering).\nmemory: Whether to use memory-based sampling that stores past positions. Memory-based samplers can be more efficient for high-dimensional problems. Defaults to true.\nN₀: Initial memory size for memory-based samplers. Should be ≥ n_chains + n_hot_chains. Defaults to 2 * n_chains + n_hot_chains.\nmemory_size: Maximum number of positions retained per chain in memory. The effective total stored positions is   memory_size * (n_chains + n_hot_chains). Defaults to 1001. Larger sizes can improve proposal diversity but   increase memory usage. Set with consideration of available RAM and expected run length.\nmemory_refill: Whether to refill memory when full, will replace from the start. Defaults to true.\nmemory_thin_interval: Thinning interval for memory updates. If > 0, only every memory_thin_interval-th position is stored in memory.\nadapt: Whether to enable adaptive behavior during warm-up (if the sampler supports it). Defaults to true.\ninitial_position: Starting positions for chains. Can be nothing (random initialization), or a vector of parameter vectors. If the provided vector is smaller than n_chains + n_hot_chains, it will be expanded; if larger and memory=true, excess positions become initial memory. Defaults to nothing.\nparallel: Whether to evaluate initial log-densities in parallel. Useful for expensive models. Defaults to false.\nmax_temp_pt: Maximum temperature for parallel tempering. Defaults to 2*sqrt(dimension).\nmax_temp_sa: Maximum temperature for simulated annealing. Defaults to max_temp_pt.\nα: Temperature ladder spacing parameter. Controls the geometric spacing between temperatures. Defaults to 1.0.\nannealing: Whether to use simulated annealing (temperature decreases over time). Defaults to false.\nannealing_steps: Number of annealing steps. Defaults to annealing ? num_warmup : 0.\nsilent: Suppress informational logging during initialization (e.g., initial position adjustments and   memory setup) when true. Defaults to false.\ntemperature_ladder: Pre-defined temperature ladder as a vector of vectors. If provided, overrides automatic temperature ladder creation. Defaults to create_temperature_ladder(n_chains, n_hot_chains, α, max_temp_pt, max_temp_sa, annealing_steps).\nkwargs...: Additional keyword arguments (unused in this method)\n\nReturns\n\nsample: DifferentialEvolutionSample containing initial positions and log-densities\nstate: Initial state (DifferentialEvolutionState) ready for sampling\n\nExamples\n\nusing DEMetropolis, Random, Distributions\n\n# Setup\nrng = Random.default_rng()\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\nsampler = deMCzs()\n\n# Basic initialization with default settings\nsample, state = step(rng, model_wrapper, sampler)\n\n# Custom number of chains with memory disabled\nsample2, state2 = step(rng, model_wrapper, sampler; n_chains=10, memory=false)\n\n# With custom initial positions\ninit_pos = [randn(2) for _ in 1:8]\nsample3, state3 = step(rng, model_wrapper, sampler; initial_position=init_pos)\n\nNotes\n\nFor non-memory samplers, n_chains should typically be ≥ dimension for good mixing\nMemory-based samplers can work effectively with fewer chains than the problem dimension\nThe function handles dimension mismatches and provides informative warnings\nInitial log-densities are computed automatically for all starting positions\nWhen using parallel tempering (n_hot_chains > 0), only the cold chains (first n_chains) are returned in the sample, but all chains participate in the sampling process\nMemory-based samplers with parallel tempering may issue warnings since hot chains typically aren't necessary when using memory\n\nSee also sample from AbstractMCMC, deMC, deMCzs, DREAMz.\n\n\n\n\n\n","category":"function"},{"location":"#AbstractMCMC.step_warmup","page":"DEMetropolis Documentation","title":"AbstractMCMC.step_warmup","text":"step_warmup(rng, model_wrapper, sampler, state; parallel=false, kwargs...)\n\nPerform a single MCMC step during the warm-up (adaptive) phase.\n\nDuring warm-up, this function performs the same sampling as step but also updates adaptive parameters. For subspace samplers, it adapts crossover probabilities based on the effectiveness of different parameter subsets.\n\nArguments\n\nrng: Random number generator\nmodel_wrapper: LogDensityModel containing the target log-density function\nsampler: Adaptive differential evolution sampler\nstate: Current state including adaptive parameters\n\nKeyword Arguments\n\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Useful if memory has grown too large.\nparallel: Whether to run chains in parallel using threading. Defaults to false.\nkwargs...: Additional keyword arguments passed to update functions\n\nReturns\n\nsample: DifferentialEvolutionSample containing new positions and log-densities\nnew_state: Updated state with adapted parameters for the next iteration\n\nExample\n\nusing DEMetropolis, Random, Distributions\n\n# Setup for warmup step example\nrng = Random.default_rng()\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\nsampler = DREAMz()\n\n# Initialize state (this would typically be done by AbstractMCMC.sample)\n# sample, new_state = step_warmup(rng, model_wrapper, sampler, state; parallel=false)\n\nSee also step, fix_sampler.\n\n\n\n\n\nstep_warmup(rng, model_wrapper, sampler, state; kwargs...)\n\nPerform a single MCMC step during warm-up for composite samplers.\n\nFor composite samplers, this function randomly selects one of the component update methods, performs a warm-up step with that method, and updates the corresponding adaptive state while preserving other component states.\n\nArguments\n\nrng: Random number generator\nmodel_wrapper: LogDensityModel containing the target log-density function\nsampler: Composite differential evolution sampler\nstate: Current state with composite adaptive parameters\n\nKeyword Arguments\n\nupdate_memory: Whether to update the memory with new positions (for memory-based samplers). Defaults to true. Useful if memory has grown too large.\nkwargs...: Additional keyword arguments passed to component update functions\n\nReturns\n\nsample: DifferentialEvolutionSample containing new positions and log-densities\nnew_state: Updated state with adapted parameters for the selected component\n\nSee also step_warmup, setup_sampler_scheme.\n\n\n\n\n\n","category":"function"},{"location":"#DEMetropolis.fix_sampler","page":"DEMetropolis Documentation","title":"DEMetropolis.fix_sampler","text":"fix_sampler(sampler::AbstractDifferentialEvolutionSampler, adaptive_state::AbstractDifferentialEvolutionAdaptiveState)\n\nFix adaptive parameters of a sampler to their current adapted values.\n\nFor non-adaptive samplers, returns the sampler unchanged. For adaptive samplers, returns a new sampler with the adaptive parameters fixed to their current values in the adaptive_state.\n\nArguments\n\nsampler: The differential evolution sampler to fix\nadaptive_state: The adaptive state containing current parameter values\n\nReturns\n\nA sampler with fixed (non-adaptive) parameters\n\nExample\n\nusing DEMetropolis, Random, Distributions\n\n# This function is typically used after warmup/adaptation phase\n# fixed_sampler = fix_sampler(adaptive_sampler, state.adaptive_state)\n\nSee also fix_sampler_state.\n\n\n\n\n\n","category":"function"},{"location":"#DEMetropolis.fix_sampler_state","page":"DEMetropolis Documentation","title":"DEMetropolis.fix_sampler_state","text":"fix_sampler_state(sampler::AbstractDifferentialEvolutionSampler, state::DifferentialEvolutionState)\n\nFix adaptive sampler parameters and return a corresponding non-adaptive state.\n\nTakes an adaptive sampler and state, fixes the sampler's adaptive parameters to their current values, and returns both the fixed sampler and a simplified state without adaptive components.\n\nArguments\n\nsampler: The differential evolution sampler (potentially adaptive)\nstate: The current sampler state (DifferentialEvolutionState)\n\nReturns\n\nfixed_sampler: Sampler with adaptive parameters fixed to current values\nfixed_state: State without adaptive components\n\nExample\n\nusing DEMetropolis, Random, Distributions\n\n# This function is typically used after warmup/adaptation phase\n# fixed_sampler, fixed_state = fix_sampler_state(sampler, state)\n\nSee also fix_sampler.\n\n\n\n\n\n","category":"function"},{"location":"#Convergence-and-Stopping-Criteria","page":"DEMetropolis Documentation","title":"Convergence and Stopping Criteria","text":"","category":"section"},{"location":"#DEMetropolis.r̂_stopping_criteria","page":"DEMetropolis Documentation","title":"DEMetropolis.r̂_stopping_criteria","text":"r̂_stopping_criteria(rng, model, sampler, samples, state, iteration; kwargs...)\n\nStopping criterion based on the Gelman-Rubin diagnostic (R̂).\n\nSampling continues until the R̂ value for all parameters falls below maximum_R̂, indicating convergence across chains. This function is designed to be used as the N_or_isdone argument in AbstractMCMC.sample for adaptive stopping.\n\nThe diagnostic is computed on the last half of the collected samples to focus on the stationary portion of the chains.\n\nArguments\n\nrng: Random number generator (unused but required by AbstractMCMC interface)\nmodel: The model being sampled (unused but required by interface)\nsampler: The differential evolution sampler (unused but required by interface)\nsamples: Vector of collected samples from all chains\nstate: Current sampler state (unused but required by interface)\niteration: Current iteration number\n\nKeyword Arguments\n\ncheck_every: Frequency (in iterations) for checking R̂ values. Defaults to 1000.\nmaximum_R̂: Maximum acceptable R̂ value for convergence. Defaults to 1.2.\nmaximum_iterations: Maximum number of iterations before forced stopping. Defaults to 100000.\nminimum_iterations: Minimum iterations before convergence checking begins. Defaults to 0.\n\nReturns\n\ntrue if sampling should stop (converged or maximum iterations reached)\nfalse if sampling should continue\n\nExample\n\nusing DEMetropolis, AbstractMCMC, Random, Distributions\n\n# Create a simple model\nmodel_wrapper(θ) = logpdf(MvNormal([0.0, 0.0], I), θ)\n\n# Setup sampler\nsampler = deMCzs()\n\n# Use with adaptive stopping criterion\nrng = Random.default_rng()\nchains = sample(rng, model_wrapper, sampler, r̂_stopping_criteria;\n               n_chains=4, check_every=500, maximum_R̂=1.1)\n\nSee also MCMCDiagnosticTools.rhat, deMCzs, DREAMz.\n\n\n\n\n\n","category":"function"},{"location":"#Output","page":"DEMetropolis Documentation","title":"Output","text":"","category":"section"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"The output format can be modified with chain_type, the supported options are Chains from MCMCChains, Any which returns the basic DEMetropolis.DifferentialEvolutionSample, and the default option DifferentialEvolutionOutput. If save_final_state = true the format will be (sample::requested format, final_state). If run in parallel using step(model, sampler, parallel_option, n_its, n_meta_chains; n_chains = n_chains) the meta chains and DE chains will be merged into one dimension for both Chains and DifferentialEvolutionOutput, if the final state is saved it will be a vector of length n_meta_chains containing the final state for each.","category":"page"},{"location":"#DEMetropolis.DifferentialEvolutionOutput","page":"DEMetropolis Documentation","title":"DEMetropolis.DifferentialEvolutionOutput","text":"DifferentialEvolutionOutput{T <: Real}\n\nContainer for differential evolution MCMC sampling results.\n\nFields\n\nsamples::Array{T, 3}: Three-dimensional array of parameter samples with dimensions (iterations, chains, parameters). Each sample represents a point in parameter space from the MCMC chain.\nld::Matrix{T}: Two-dimensional matrix of log-density values with dimensions (iterations, chains). Contains the log-probability density evaluated at each corresponding sample point.\n\nType Parameters\n\nT <: Real: Numeric type for the samples and log-density values (typically Float64).\n\nExamples\n\n# Access samples from the output\noutput = sample(model, sampler, n_samples)\nparameter_samples = output.samples  # Shape: (n_samples, n_chains, n_params)\nlog_densities = output.ld          # Shape: (n_samples, n_chains)\n\n# Extract samples for a specific chain\nchain_1_samples = output.samples[:, 1, :]  # All samples from chain 1\n\n\n\n\n\n","category":"type"},{"location":"#Index","page":"DEMetropolis Documentation","title":"Index","text":"","category":"section"},{"location":"","page":"DEMetropolis Documentation","title":"DEMetropolis Documentation","text":"","category":"page"},{"location":"tutorial/#Sampling-from-multimodal-distributions","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"","category":"section"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"Let say you have a multimodal distribution, for example a mixture of two Gaussians. DEMetropolis implements differential evolution MCMC samplers (including deMC-zs and DREAMz) that are designed to sample from such distributions efficiently. Roughly these samplers work by generating new proposals based on many separate chains (or a history of sampled chains). In theory this allows the sampler to easily jump between modes of the distribution.","category":"page"},{"location":"tutorial/#Multimodal-Distributions","page":"Sampling from multimodal distributions","title":"Multimodal Distributions","text":"","category":"section"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"First we need to implement a multimodal distribution. We'll use a mixture of three Gaussians for one parameter and two LogNormal distributions for another, making this a 2D distribution. We can easily implement this using the Distributions package, which underpins most of the functionality in DEMetropolis.","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"using Distributions\n\nα_mixed_dist = MixtureModel([\n    Normal(-5.0, 0.5),\n    Normal(0.0, 0.5),\n    Normal(5.0, 0.5)\n], [1/4, 1/4, 1/2]);\n\nβ_mixed_dist = MixtureModel([\n    LogNormal(-0.5, 0.5),\n    LogNormal(1.75, 0.25)\n], [1/6, 5/6]);\n\nfunction multimodal_ld(θ)\n    (; α, β) = θ\n    logpdf(α_mixed_dist, α) +\n        logpdf(β_mixed_dist, β)\nend","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"We can also transform our log density function, so we can provide real-valued inputs. This is much easier to work with.","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"using TransformedLogDensities, TransformVariables\ntransformation = as((α = asℝ, β = asℝ₊))\ntransformed_ld = TransformedLogDensity(transformation, multimodal_ld)","category":"page"},{"location":"tutorial/#Sampling-with-DEMetropolis","page":"Sampling from multimodal distributions","title":"Sampling with DEMetropolis","text":"","category":"section"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"Now let's use DEMetropolis to sample from this multimodal distribution. Here we use the DREAMz sampler, which is well-suited for exploring complex, multimodal spaces. We increase the number of chains to allow the sampler to explore the distribution more effectively.","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"using DEMetropolis, AbstractMCMC, Random\n\nmodel = AbstractMCMC.LogDensityModel(transformed_ld)\n\nRandom.seed!(1234)\n\n# Sample using DREAMz with adaptive stopping based on convergence\ndreamz = DREAMz(model, 10000; n_chains = 6, progress = true);","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"Other implementations of the differential evolution MCMC algorithm are available in DEMetropolis.jl, such as deMC and deMCzs, which can be used similarly.","category":"page"},{"location":"tutorial/#Custom-Scheme","page":"Sampling from multimodal distributions","title":"Custom Scheme","text":"","category":"section"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"DREAMz can be further customized. For example, we could include snooker updates alongside the DREAMz-like subspace sampling.","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"You can also modify aspects of the implemented sampling, for example tell DREAMz to use non-memory-based sampling with DREAMz(...; memory = false), or you can define your own sampler scheme for more control over the sampling process.","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"# Create a custom sampler scheme combining different update types\ncustom_sampler = setup_sampler_scheme(\n    setup_subspace_sampling(), # a DREAM-like sampler that uses subspace sampling\n    setup_snooker_update(deterministic_γ = false), # a snooker update for better exploration\n    setup_de_update(); # standard DE update\n    w = [0.6, 0.2, 0.2] # weights for each update type\n);\n\n# Sample using AbstractMCMC.sample with custom stopping criteria\ncustom_results = sample(\n    model,\n    custom_sampler,\n    r̂_stopping_criteria;\n    check_every = 1000,\n    maximum_R̂ = 1.05,\n    n_chains = 4,\n    memory = true,\n    parallel = true,\n    num_warmup = 10000,\n    chain_type = DifferentialEvolutionOutput\n);","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"You can also define your own samplers for more specialized use cases by extending the abstract types.","category":"page"},{"location":"tutorial/#Interpreting-Results","page":"Sampling from multimodal distributions","title":"Interpreting Results","text":"","category":"section"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"After running the sampler, you will have a collection of samples from the target distribution. These samples can be used to estimate summary statistics, credible intervals, and to assess the quality of your sampling.","category":"page"},{"location":"tutorial/#Assessing-Sampler-Performance:-ESS-and-R-hat","page":"Sampling from multimodal distributions","title":"Assessing Sampler Performance: ESS and R-hat","text":"","category":"section"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"To evaluate how well your sampler is performing, you can compute the effective sample size (ESS) and the R-hat diagnostic. These metrics help you determine if your chains have mixed well and if your estimates are reliable.","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"Effective Sample Size (ESS): This measures the number of independent samples your chains are equivalent to. Higher ESS values indicate more reliable estimates.\nR-hat Diagnostic: Also known as the Gelman-Rubin statistic, R-hat compares the variance within each chain to the variance between chains. Values close to 1 suggest good mixing and convergence; values much greater than 1 indicate potential problems.","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"Below is an example of how to compute these diagnostics for the DEMetropolis samplers using MCMCDiagnosticTools:","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"using Statistics, MCMCDiagnosticTools\n\n# Compute diagnostics for DREAMz results\ness_val = ess(dreamz.samples) ./ size(dreamz.samples, 1)\nrhat_val = maximum(rhat(dreamz.samples))\n\nprintln(\"DREAMz diagnostics:\")\nprintln(\"  ESS per iteration: $ess_val\")\nprintln(\"  R-hat: $rhat_val\")","category":"page"},{"location":"tutorial/#Summarizing-Posterior-Samples","page":"Sampling from multimodal distributions","title":"Summarizing Posterior Samples","text":"","category":"section"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"Once you have confirmed good mixing and convergence, you can summarize your posterior samples. For each parameter, you may want to compute the median and a credible interval (such as the 90% interval):","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"\n# Flatten the samples across all chains and iterations for each parameter\nn_params = size(custom_results.samples, 3)\n\nreduced_samples = custom_results.samples[(size(custom_results.samples, 1) ÷ 2):size(custom_results.samples, 1), :, :]\ntransformed_samples = [\n    transform(transformation, vec(reduced_samples[iteration, chain, :])) for \n        iteration in axes(reduced_samples, 1),\n        chain in axes(reduced_samples, 2)\n][:]\nflat_samples = [vec([transformed_samples[i][param] for i in axes(transformed_samples, 1)]) for param in 1:n_params]\n\nmed = median(flat_samples[1])\nq05, q95 = quantile(flat_samples[1], [0.05, 0.95])\nprintln(\"Parameter α: median = $med, 90% CI = ($q05, $q95)\")\nprintln(\"True α: median = $(median(α_mixed_dist)), 90% CI = ($(quantile(α_mixed_dist, 0.05)), $(quantile(α_mixed_dist, 0.95)))\")\n\nmed = median(flat_samples[2])\nq05, q95 = quantile(flat_samples[2], [0.05, 0.95])\nprintln(\"Parameter β: median = $med, 90% CI = ($q05, $q95)\")\nprintln(\"True β: median = $(median(β_mixed_dist)), 90% CI = ($(quantile(β_mixed_dist, 0.05)), $(quantile(β_mixed_dist, 0.95)))\")","category":"page"},{"location":"tutorial/","page":"Sampling from multimodal distributions","title":"Sampling from multimodal distributions","text":"For more details, see the DEMetropolis Documentation and Customizing your sampler.","category":"page"}]
}
